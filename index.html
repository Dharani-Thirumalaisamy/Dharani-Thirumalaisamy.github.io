
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>CSYE 7374 : FINAL PROJECT REPORT</title>
  <script src="../../bower_components/webcomponentsjs/webcomponents-lite.js"></script>
  <link rel="import" href="../../elements/codelab.html">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <style is="custom-style">
    body {
      font-family: "Roboto",sans-serif;
      background: var(--google-codelab-background, #F8F9FA);
    }
  </style>
  
</head>
<body unresolved class="fullbleed">

  <google-codelab title="CSYE 7374 : FINAL PROJECT REPORT"
                  environment="web"
                  feedback-link="github.com/Dharani-Thirumalaisamy">
    
      <google-codelab-step label="Project Topic" duration="0">
        <p><strong>Gun and Knife Identification In Images</strong></p>


      </google-codelab-step>
    
      <google-codelab-step label="Authors" duration="0">
        <ul>
<li><strong> Dharani Thirumalaisamy </strong></li>
<li><strong> Pramod Nagare </strong></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Objective" duration="0">
        <p>As we know we humans are very good at detecting and identifying different objects in an image.<br>Our visual system is very fast and accurate with complex tasks like detecting objects and identifying obstacles,<br>which leads us to take several actions like run, walk, talk, jump, etc.<br>But when it comes to perform the same using Artificial Intelligence technique it becomes very tough.<br>The perform of autonomous driving cars, robotics, tracking object, face detections and many more applications,<br>depends on the object detection algorithm it is based on. As object detection serves the base for such innovative and<br>vast range of applications, we as a students of cognitive computing class would like to work on object detection in an image.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Dataset" duration="0">
        <p>We have readily available dataset for knife images from :<br><a href="http://kt.agh.edu.pl/matiolanski/KnivesImagesDatabase/" target="_blank">Download Knives</a></p>
<p>Also, we have images of different categories of guns :<br><a href="http://www.imfdb.org/wiki/Category:Gun" target="_blank">Download Guns</a></p>
<p>To get more models and shapes of knife, we used falcon :<br><a href="https://chrome.google.com/webstore/detail/falconio/jkebldamonadkdoliopafdngmkjmciie?hl=en" target="_blank">Download Falcon Extension</a></p>
<p>Knife and Guns images were scraped from the above two sites.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Flowchart" duration="0">
        <p><img alt="pipeline" src="img\6ef64372da92da62.png"></p>
<p>Description:</p>
<ul>
<li>User accesses flask application from his browser.</li>
<li>User uploads image through flask application to detect knife or gun.</li>
<li>Then the image will access a EC2 instance on which flask application is running.<br>This in turn calls the object detection model that is running in the back-end of the application.</li>
<li>If any gun or knife is detected in an image, using IAM RDE, EC2 instance will publish a SNS message which will trigger an AWS lambda function<br>which eventually triggers an email to the server side user.</li>
<li>Also the image detected in the image will be reported back to the user.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Data Scraping" duration="0">
        <ul>
<li>Libraries Imported for web scraping :<br><img alt="libraries" src="img\9cd23d0ec8583734.png"><br>These are the libraries that were used to scrape the images from the website.</li>
<li>Parsing the URL of guns website.<br>There are 24 different categories of gun. So the code will loop through all those categories to scrape all the images.<br><img alt="parsing" src="img\5ff02d4c7f9b7711.png"></li>
<li>In this the knife URL is passed and the .rar file is extracted and stored in a local directory.<br><img alt="unrar" src="img\9ec867d057314cda.png"></li>
<li>The images that was scraped different height and width. So in order to make height and width the same these lines of code had to be run.<br>Here every image&#39;s height and width is made 200x200 for uniformity.<br><img alt="dimension" src="img\4f1853a81cdfed43.png"></li>
</ul>
<p>After the images are extracted, the next step is to pass it through an object detection network.<br>There are many networks that can be worked on :</p>
<ul>
<li>Sliding window</li>
<li>R-CNN</li>
<li>Fast R-CNN</li>
<li>Faster R-CNN</li>
<li>Single Shot Detector</li>
<li>YOLO(Yolo9000, Yolov2,Yolov3)</li>
<li>MobileNet</li>
<li>Inception Network</li>
</ul>
<p>Every network listed above can be implemented from scratch, but due to time constraint and computational complexity, we have done transfer learning in our project.<br>So for every network that we tried, we ran the official architecture on our custom dataset(knife and gun) for the pre-trained weights and configuration( which was altered for our need) and got the weights for our dataset on which the images/video was trained.</p>
<p>The architectures that we chose are :<br></p>
<ul>
<li>Yolov2</li>
<li>SSD on mobilenet</li>
<li>Faster R-CNN</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Object Detection Using Yolov2" duration="0">
        <ul>
<li>Now that we have the images with us, the first step to do is annotation of images. These images should have a boundary box and also labelled for the network to classify them. To do so, there are 2 GUI&#39;s :<br><br><ul>
<li>LabelImage Tool box</li>
<li>BBox Label Tool box<br>For Yolo, labelImg tool box is used.<br>(Just run labelImg.py from terminal)<br><img alt="labelimg" src="img\1eaa981281cb26a9.png"></li>
</ul>
</li>
</ul>
<p>This is what it looks like.<br>In ‘Open Dir&#39; the directory that contains the images should be given and to store the output files, the path can be given in ‘Changes Save Dir&#39;.</p>
<p>In this method, the annotation files should be in .txt format. There is an option to choose YOLO format in this GUI.</p>
<p>This will give text files for each image of the format :<br>Label_index , x_min,y_min,x_max,y_max</p>
<p>In our case, Label_Index 0 is Gun and 1 is Knife.</p>
<ul>
<li>Once annotations are done for all the images, create a obj.names file with gun and knife in it.<br><img alt="annot" src="img\936fe2275399d0dc.png"></li>
<li>Install darknet which is the framework for YOLO.<br><img alt="darknet" src="img\f758f16f89162c96.png"></li>
<li>In that darknet, there will be a cfg folder which will have all the weights.<br>For this project Yolov2.cfg was used. Copy the contents in that configuration file to another configuration file and<br>name it the way u want it with .cfg extension.<br><img alt="cfg" src="img\af01f9ffa6e92ae9.png"></li>
<li>Change the number of filters in the last Convolutional layer to (num_of_classes+5)*5.<br>In this case 35 and also change the number of classes in the next layer.</li>
<li>Create 2 separate text files named train.txt and test.txt with names of all the training images and testing  images. Copy this into darknet folder.</li>
<li>Download the pre-trained weights file for Yolo <a href="https://pjreddie.com/darknet/yolov2/" target="_blank">download darknet19_448.conv.23</a> from official website.</li>
<li>Create 2 other files named obj.data with details about where the test, train data is stored, about where to save the weights and also form<br>where to get information about labels(obj.names) inside cfg folder in darknet.<br><img alt="obj" src="img\58ef299404eca57b.png"><img alt="weights" src="img\41d1ef9706e459e8.png"><br>The weight file here can be downloaded from <a href="https://console.cloud.google.com/storage/browser/yoloweights" target="_blank">Download Yolo Weights</a></li>
<li>Once all these files are ready, run the following command to train your network for custom dataset and get your weights.<br>Run this command from darknet folder :<br><strong> ./darknet detector train cfg/obj.data cfg/yolo-obj.cfg darknet19_448.conv.23 </strong></li>
</ul>
<p>Since YOLO architecture is complicated and complex it will take hours to train the model.<br>On CPU, it will take years. This model has 100 epochs, with batch size = 64 and subdivisions  = 8 .<br>In normal CPU, each epoch takes 30 hours, so for training 100 epochs it would take a long time.<br>So I trained my model using NVIDIA titan Graphics Card. It took 3 hours to train the complete network.<br><img alt="train" src="img\3d7934add23599e8.png"><img alt="tr" src="img\9638436d28e54bf7.png"><img alt="test" src="img\b099c5d0978c2fea.png"><img alt="te" src="img\197e9d1c7b025756.png"></p>
<aside class="warning"><p>Unfortunately, the IoU values that we got was very less and loss didn&#39;t decrease. So, the output that we got didn&#39;t predict anything.<br><img alt="g" src="img\c067de7eddd2bb17.png"></p>
</aside>
<aside class="special"><p>With this as the case, we decided to switch to another network , which is SSD_mobilenet.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Object Detection Using SSD MobileNet" duration="0">
        <p>Single Shot Detector is another model used for object detection. It is relatively fast to train compared to YOLO.</p>
          <p>Before starting with the implementation, tensorflow environment should be setup properly to execute the files in this model.<br>For that refer <a href="https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10#appendix-common-errors" target="_blank">Tensorflow</a><br>Once the environment is setup, the model implementation can be done.</p>
          <p>To implement this :</p>
<ul>
<li>Just like how data annotation is done for YOLO, we need to do annotation in this case too but the only difference Is the format.<br>For SSD we .csv format is required.</li>
<li>The same tool box can be used to do the annotation.</li>
<li>The tool box provided only .txt and .xml format. So first do the annotation in .xml format for all the images.<br>Split them into train and test images. Run xml_to_csv.py separately for test and train images to get train_lables.csv and test_labels.csv.<br><br></li>
</ul>
<p><img alt="xml" src="img\433d230c494e3645.png"><br>This is how annotation xml file looks.</p>
<p><img alt="csv" src="img\4dfb39a3f2d36f34.png"><br>This is csv file format.</p>
<ul>
<li>Once this is done, download SSD_mobilenet_v2_coco.cfg from <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" target="_blank">Download models</a></li>
<li>Edit the cfg files. Change the training image path, testing image path, training annotation path, testing annotation path.</li>
<li>Change the number of classes too.</li>
<li>Once all these are done, create a training folder inside which create a .pbtxt file with classes names on it.</li>
<li>Now run this command from the terminal :<br><strong>python train.py –logtostderr –train_dir=training/ –pipeline_config_path=training/ssd_mobilenet_v2_coco.config<br></strong><br>Training will take some time, but the weights will get saved for every 5 steps.<br><img alt="t" src="img\fcf14d21a9f5b9fc.png"></li>
</ul>
<p><strong> python export_inference_graph.py –input_type image_tensor</strong></p>
<pre><code>                             --pipeline_config_path training/ssd_mobilenet_v2_coco.config 
                             --trained_checkpoint_prefix training/model.ckpt-(highest value) 
                             --output_directory inference_graph
</code></pre>
<p>Run this command with the highest value of weight file that Is generated.<br><img alt="ssd" src="img\cb8c3792ae636612.png"></p>
<p>Once this is done , a .pb file will be generated. Now run object_detection_image.py file from python idle and check the output.</p>
<aside class="warning"><p>In this, Knifes are getting detected as knifes but guns are also getting detected as knifes or sometimes not being detected at all.<br>So that is the drawback about this model.<br><img alt="i" src="img\86ae17a5c64be127.png"><br></p>
</aside>
<aside class="special"><p>To overcome this problem, we tried another network : Faster R-CNN.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Object Detection Using Faster R-CNN and Inceptionv2" duration="0">
        <p>Faster R-CNN is a one of the method for object detection. Though it is not the state-of-art, it performs very well and also it trains faster than SSD.</p>
<p>As the training portion of Faster R-CNN is similar to that of SSD, steps a to steps g are the same except for step D.<br>In this, instead of downloading SSD weights, <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" target="_blank">Download models</a></p>
<p>After setting up the environment and other files, train the model using :<br><strong>python train.py –logtostderr –train_dir=training/ –pipeline_config_path=training/faster_rcnn_inception_v2_pets.config</strong></p>
<p>Just like SSD, weights file will get updated after some steps and run the below command with highest weight file.<br><strong>python export_inference_graph.py –input_type image_tensor</strong></p>
<pre><code>                                --pipeline_config_path training/ faster_rcnn_inception_v2_pets.config 
                                --trained_checkpoint_prefix training/model.ckpt-(highest value) 
                                --output_directory inference_graph&lt;/b&gt;
</code></pre>
<p>Training for a long time decreases loss rate to 0.02 and below.<br><img alt="loss" src="img\178ecc2bd45acb35.png"><img alt="l" src="img\f7b7b116ada780e5.png"></p>
<aside class="special"><p>The output that we got is better than the other two models. So, we will be moving forward with this model as our final one.<br></p>
</aside>
<p><img alt="op" src="img\413ab0de72f5483f.png"><img alt="op1" src="img\146ba28ef721000b.png"><img alt="op2" src="img\1f4cef2dc757fd2.png"></p>
        </google-codelab-step>
    
        <google-codelab-step label="Trigger Implementation" duration="0">   
<p>Now that we have our best model out of 3, the next part is to trigger an alarm or email or SMS to someone who is monitoring it.<br>Here, in our project we have implemented email trigger.<br>So when user inputs an image and if our model detects it as knife or gun, the <a href="https://aws.amazon.com/lambda/features/" target="_blank">lambda function</a><br>provided by Amazon will get triggered and will send an email to the person in-charge of it.<br><img alt="trigger" src="img\4d3df5adb2467e84.png"></p>
<p>For implementing trigger, Amazon&#39;s SES service is used. In this we are mentioning that we need to trigger an email based on an event which is detection of<br>a gun or a knife.<br><img alt="op3" src="img\ac7ddba1cf62135d.png"></p>
        </google-codelab-step>
        
        <google-codelab-step label="Flask Application" duration="0">
<p>The important part is that people should be able to upload images or video. In order to accommodate, a flask application is developed to upload images.<br><img alt="fl" src="img\b74799add56605d.png"><br>First we are asking the user to input an image. The template for the web page is called from index.html file.<br><img alt="temp" src="img\61464fccf3d73cd9.png"></p>
<p>The image is read in numpy array format which is later converted to image. This image is given as input to ObjectDetection.py file which will detect<br>the object in the uploaded image.<br><img alt="picf" src="img\fee36910caf7bff4.png"><img alt="p" src="img\381ee4c0ab5db7c1.png"><img alt="q" src="img\13249c4984fd87ce.png"><img alt="w" src="img\e101eea658293010.png"></p>ObjectDetector.py is same as object_detection_image.py that we used in SSD and Faster R-CNN model.</p>
          </google-codelab-step>
        
          <google-codelab-step label="References" duration="0">
        
<p><em>[a]</em><a href="https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10#appendix-common-errors" target="_blank"><em>TensorFlow Setup</em></a><em><br>[b]</em><a href="https://www.arunponnusamy.com/yolo-object-detection-opencv-python.html" target="_blank"><em>Yolo Reference 1</em></a><em><br>[c]</em><a href="https://github.com/ManivannanMurugavel/YOLO-Annotation-Tool" target="_blank"><em>Yolo Reference 2</em></a><em><br>[d]</em><a href="] https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects" target="_blank"><em>Yolo Reference 3</em></a><em><br>[e]</em><a href="https://github.com/pjreddie/darknet/issues/382" target="_blank"><em>Darknet issues</em></a><em><br>[f]</em><a href="https://medium.com/coinmonks/detecting-custom-objects-in-images-video-using-yolo-with-darkflow-1ff119fa002f" target="_blank"><em>Custom Object Detection</em></a><em><br>[g]</em><a href="https://pjreddie.com/darknet/" target="_blank"><em>Darknet official</em></a><em><br>[h]</em><a href="https://medium.com/coinmonks/detecting-custom-objects-in-images-video-using-yolo-with-darkflow-1ff119fa002f" target="_blank"><em>Darknet Github</em></a><em><br>[i]</em><a href="https://github.com/tzutalin/labelImg" target="_blank"><em>Annotation-Tool</em></a></p>


      </google-codelab-step>
    
  </google-codelab>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-49880327-14', 'auto');

    (function() {
      var gaCodelab = '0';
      if (gaCodelab) {
        ga('create', gaCodelab, 'auto', {name: 'codelab'});
      }

      var gaView;
      var parts = location.search.substring(1).split('&');
      for (var i = 0; i < parts.length; i++) {
        var param = parts[i].split('=');
        if (param[0] === 'viewga') {
          gaView = param[1];
          break;
        }
      }
      if (gaView && gaView !== gaCodelab) {
        ga('create', gaView, 'auto', {name: 'view'});
      }
    })();
  </script>

</body>
</html>
